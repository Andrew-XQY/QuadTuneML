{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a563bba6-aaf1-4fc3-ad73-e852c52ae0a0",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b268446f-eac8-45de-bdd2-7adfe7f41952",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "config = load_config(\"config.yaml\")\n",
    "\n",
    "# read data source\n",
    "df = pd.read_csv(\"data/batch_results.csv\")\n",
    "cols = ['mean_x', 'mean_y', 'sigma_x', 'sigma_y', 'emittance_x', 'emittance_y', 'transmission']\n",
    "data = pick_from_list_str(df, cols, index=-1)\n",
    "# convert all string to float\n",
    "data = convert_strings_to_float(data)\n",
    "# to log scale\n",
    "data = log_scale_df(data, cols=['emittance_x', 'emittance_y', 'transmission'])\n",
    "# remove outliers (thresholding percentile)\n",
    "cols = ['mean_x', 'mean_y', 'sigma_x', 'emittance_y', 'transmission']\n",
    "data = add_valid_flag(data, cols, 0.01)\n",
    "data = data[data['valid'] == 1].drop(columns=['valid']).reset_index(drop=True)\n",
    "# normalized range\n",
    "data = normalize_min_max(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e6641c-b66b-40f9-9b54-7c49075918b8",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553b9702-8107-4357-bff7-3019677673a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "import tensorflow as tf\n",
    "\n",
    "# ——— Model ———\n",
    "\n",
    "def build_model(config):\n",
    "    inp = tf.keras.Input(shape=(len(config['input_cols']),))\n",
    "    \n",
    "    # 1st block\n",
    "    x = tf.keras.layers.Dense(256, use_bias=False)(inp)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    \n",
    "    # 2nd block\n",
    "    x = tf.keras.layers.Dense(128, use_bias=False)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    \n",
    "    # 3rd block\n",
    "    x = tf.keras.layers.Dense(128, use_bias=False)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(32, use_bias=False)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "\n",
    "    # linear output for full-range regression\n",
    "    out = tf.keras.layers.Dense(len(config['target_cols']))(x)\n",
    "    \n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=config['learning_rate'])\n",
    "    model = tf.keras.Model(inp, out)\n",
    "    model.compile(\n",
    "        optimizer=opt,\n",
    "        loss=tf.keras.losses.Huber(),\n",
    "        # loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = build_model(config)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84630a0d-e23f-4c72-9929-bac0047d4853",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ——— Data prep ———\n",
    "X = data[config['input_cols']].to_numpy(dtype='float32')\n",
    "y = data[config['target_cols']].to_numpy(dtype='float32')\n",
    "\n",
    "n = X.shape[0]\n",
    "idx = tf.random.shuffle(tf.range(n), seed=config['seed'])\n",
    "n_test = int(config['test_size'] * n)\n",
    "n_val  = int(config['val_size']  * n)\n",
    "\n",
    "test_idx  = idx[:n_test]\n",
    "val_idx   = idx[n_test:n_test+n_val]\n",
    "train_idx = idx[n_test+n_val:]\n",
    "\n",
    "def make_ds(indices):\n",
    "    return (tf.data.Dataset\n",
    "              .from_tensor_slices((tf.gather(X, indices),\n",
    "                                   tf.gather(y, indices)))\n",
    "              .batch(config['batch_size'])\n",
    "              .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "train_ds, val_ds, test_ds = map(make_ds, (train_idx, val_idx, test_idx))\n",
    "\n",
    "\n",
    "# ——— Callbacks ———\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=config['patience'],\n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=config['model_path'],\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True\n",
    "    )\n",
    "]\n",
    "\n",
    "# ——— Train ———\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=config['epochs'],\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "model.save(config['model_path'])\n",
    "plot_history(history)\n",
    "\n",
    "# ——— Eval ———\n",
    "X_test = np.vstack([x for x, _ in test_ds])\n",
    "y_test = np.vstack([y for _, y in test_ds])\n",
    "\n",
    "# model predictions\n",
    "y_pred = model.predict(test_ds)\n",
    "\n",
    "# build and save dataframe\n",
    "df_out = pd.DataFrame(\n",
    "    np.hstack([X_test, y_test, y_pred]),\n",
    "    columns=(\n",
    "        config['input_cols']\n",
    "      + [f\"true_{c}\" for c in config['target_cols']]\n",
    "      + [f\"pred_{c}\" for c in config['target_cols']]\n",
    "    )\n",
    ")\n",
    "df_out.to_csv(config['save_to'] + \"test_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598adcb4-ae31-4413-83bb-a9253561a8ec",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ec9441-81a0-43e5-a776-a7275a084360",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "result = pd.read_csv(config['save_to']+\"test_results.csv\")\n",
    "result = clip_df(result)\n",
    "process_df(result, inputs=config['input_cols'], outputs=config['target_cols'], save_to=config['save_to'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11630bb6-0f85-4bf5-b815-81fb55156951",
   "metadata": {},
   "source": [
    "# Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2caeeb5c-ac19-435b-872a-9a0fb0aa8b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "config = load_config(\"config.yaml\")\n",
    "cols = ['mean_x', 'mean_y', 'sigma_x', 'sigma_y', 'emittance_x', 'emittance_y', 'transmission']\n",
    "\n",
    "\n",
    "temp = []\n",
    "for i in range(8):\n",
    "    df = pd.read_csv(\"data/batch_results.csv\")\n",
    "    data = pick_from_list_str(df, cols, index=i)\n",
    "    data = convert_strings_to_float(data)\n",
    "    temp.append(data)\n",
    "min_d, max_d = compute_bounds(temp)\n",
    "\n",
    "temp = []\n",
    "for i in range(8):\n",
    "    df = pd.read_csv(\"data/batch_results.csv\")\n",
    "    data = pick_from_list_str(df, cols, index=i)\n",
    "    data = convert_strings_to_float(data)\n",
    "    temp.append(plot_distributions(data, bins=200, min_bounds=min_d, max_bounds=max_d, title=f'segment_{i+1}'))\n",
    "\n",
    "figures_to_gif(temp, config['save_to'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
